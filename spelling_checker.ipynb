{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbfead40",
   "metadata": {},
   "source": [
    "# Spelling correction\n",
    "\n",
    "The following files are required:\n",
    "- sentences_with_typos.txt: a file with two tab-separated columns, the first containing a numerical ID and the second containing a sentence in English\n",
    "- SUBTLEXus.txt: a file with several tab-separated columns, containing data from the SUBTLEXus lexicon, a list of words derived from a part of the SUBTLEXus corpus containing movie subtitles with their frequency counts (and other pieces of information)\n",
    "- LM_trainingCorpus.json: a file containing a pre-processed corpus in the form of a list of lists of strings, to be used to train a word-level statistical language model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a274bd",
   "metadata": {},
   "source": [
    "## Task1\n",
    "\n",
    "In the cell below you find code to run a statistical language model, add the docstrings (you already find a blueprint) to complete the first task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c603a4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "class LM(object):\n",
    "    \n",
    "    def __init__(self, corpus, ngram_size=2, bos='+', eos='#', k=1):\n",
    "        \n",
    "        \"\"\"        \n",
    "        :param corpus: List of sentences, where each sentence is a list of tokens.\n",
    "        :param ngram_size: The size of the n-grams to be used for the language model (default is bigrams).\n",
    "        :param bos: Beginning of sentence (default is '+').\n",
    "        :param eos: End of sentence (default is '#').\n",
    "        :param k: Smoothing constant for Laplace smoothing (default is 1).\n",
    "        \n",
    "        This class creates an LM object with attributes for setting the corpus, n-gram size, bos and eos tokens, \n",
    "        and Laplace smoothing constant k.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.k = k\n",
    "        self.ngram_size = ngram_size\n",
    "        self.bos = bos\n",
    "        self.eos = eos\n",
    "        self.corpus = corpus\n",
    "        self.vocab = self.get_vocab()\n",
    "        self.vocab.add(self.eos)\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        \n",
    "    def get_vocab(self):\n",
    "        \n",
    "        \"\"\"        \n",
    "        :return: A set that contains the unique tokens in given corpus (vocab).\n",
    "        \n",
    "        This method derives the vocabulary set from the input corpus by iterating through each sentence and \n",
    "        each token (element) within the selected sentence, then add the constituant elements to vocab set.\n",
    "        \"\"\"\n",
    "        \n",
    "        vocab = set()\n",
    "        for sentence in self.corpus:\n",
    "            for element in sentence:\n",
    "                vocab.add(element)\n",
    "        \n",
    "        return vocab\n",
    "                    \n",
    "    def update_counts(self):\n",
    "\n",
    "        \"\"\"\n",
    "        :return: None.\n",
    "        \n",
    "        \n",
    "        This method creates a 'counts' using defaultdict with the n-gram counts for the training corpus. The counts is\n",
    "        a nested dictionary with keys are tuples containing the first n-gram -1 words and the value is a dictionary\n",
    "        containing the next word(s) with their counts. \n",
    "        \n",
    "        The method also add padding to each sentence in the corpus before processing. Pedding is important to keep\n",
    "        track of the start and end of a sentence.\n",
    "        \n",
    "        This (counts) dictionary is useful for calculating n-gram probabilities and perplexities.\n",
    "        \"\"\"\n",
    "        \n",
    "        r = self.ngram_size - 1\n",
    "        \n",
    "        self.counts = defaultdict(dict)\n",
    "    \n",
    "        for sentence in self.corpus:\n",
    "            s = [self.bos]*r + sentence + [self.eos]\n",
    "            \n",
    "            for idx in range(self.ngram_size-1, len(s)):\n",
    "                ngram = self.get_ngram(s, idx)\n",
    "                \n",
    "                try:\n",
    "                    self.counts[ngram[0]][ngram[1]] += 1\n",
    "                except KeyError:\n",
    "                    self.counts[ngram[0]][ngram[1]] = 1\n",
    "                        \n",
    "    def get_ngram(self, s, i):\n",
    "        \n",
    "        \"\"\"        \n",
    "        :param s: Sentence represented as a list of tokens include peddings.\n",
    "        :param i: Index in the sentence where the n-gram ends.\n",
    "        :return: A tuple containing the n-gram history and target word.\n",
    "        \n",
    "        This method retrieves the n-gram at a given index in a sentence, slice it and return its  \n",
    "        history and the target as a tuple.\n",
    "        \"\"\"\n",
    "        \n",
    "        ngram = s[i-(self.ngram_size-1):i+1]\n",
    "        history = tuple(ngram[:-1])\n",
    "        target = ngram[-1]\n",
    "        return (history, target)\n",
    "    \n",
    "    def get_ngram_probability(self, history, target):\n",
    "        \n",
    "        \"\"\"        \n",
    "        :param history: A tuple containing the history of the n-gram.\n",
    "        :param target: The target token of the n-gram.\n",
    "        :return: The probability of the target token given its history.\n",
    "        \n",
    "        This method calculates the probability of a target token given its n-gram history.\n",
    "        It uses the n-gram counts and applies constant k (Laplace smoothing) to estimate the probability\n",
    "        of a word occurring in a given context.\n",
    "        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            ngram_tot = np.sum(list(self.counts[history].values())) + (self.vocab_size*self.k)\n",
    "            try:\n",
    "                transition_count = self.counts[history][target] + self.k\n",
    "            except KeyError:\n",
    "                transition_count = self.k\n",
    "        except KeyError:\n",
    "            transition_count = self.k\n",
    "            ngram_tot = self.vocab_size*self.k\n",
    "\n",
    "        return transition_count/ngram_tot \n",
    "\n",
    "    def perplexity(self, sentence):\n",
    "        \n",
    "        \"\"\"\n",
    "        :param sentence: A list of tokens representing the sentence.\n",
    "        :return: The perplexity of the input sentence.\n",
    "\n",
    "        This method computes the preplexity measure as an evaluation of a the language model's ability to predict\n",
    "        the given sentence. It does this by calculating the log probability of each n-gram using (get_ngram_probability)\n",
    "        method and then adding the log to 'probs' (list). The perplexity obtained by taking the exponent of the\n",
    "        negative average of the log probabilities 'avg_entropy'.\n",
    "        \"\"\"\n",
    "\n",
    "        r = self.ngram_size - 1\n",
    "        s = [self.bos]*r + sentence + [self.eos]\n",
    "\n",
    "        probs = []\n",
    "        for idx in range(self.ngram_size-1, len(s)):\n",
    "            ngram = self.get_ngram(s, idx)\n",
    "            probs.append(self.get_ngram_probability(ngram[0], ngram[1]))\n",
    "\n",
    "        entropy = np.log2(probs)\n",
    "        avg_entropy = -1 * (sum(entropy) / len(entropy))\n",
    "        return pow(2.0, avg_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "075ec16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6502f6a9",
   "metadata": {},
   "source": [
    "## Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d8ea744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "sentences_with_typos_df = pd.read_csv(\"CL/sentence_with_typos.txt\", sep=\"\\t\", keep_default_na=False)\n",
    "SUBTLEXus_df = pd.read_csv(\"CL/SUBTLEXus.txt\", sep=\"\\t\", keep_default_na=False, low_memory=False)\n",
    "\n",
    "with open(\"CL/LM_trainingCorpus.json\", \"r\") as f:\n",
    "    LM_training_corpus = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54541fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ID                                           sentence\n",
      "0   1           did you knoq that our fiends had a baby?\n",
      "1   2  I red that reserachers managed to deviate the ...\n",
      "2   3  I could not help but grozn in frustration when...\n",
      "3   4  I mate a quolg from old clothes for my newborn...\n",
      "4   5  he waies his wand to get the attention of the ...\n"
     ]
    }
   ],
   "source": [
    "print(sentences_with_typos_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a505f6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Word  FREQcount  CDcount  FREQlow  Cdlow   SUBTLWF  Lg10WF  SUBTLCD  Lg10CD\n",
      "0  the    1501908     8388  1339811   8388  29449.18  6.1766   100.00  3.9237\n",
      "1   to    1156570     8383  1138435   8380  22677.84  6.0632    99.94  3.9235\n",
      "2    a    1041179     8382   976941   8380  20415.27  6.0175    99.93  3.9234\n",
      "3  you    2134713     8381  1595028   8376  41857.12  6.3293    99.92  3.9233\n",
      "4  and     682780     8379   515365   8374  13387.84  5.8343    99.89  3.9232\n"
     ]
    }
   ],
   "source": [
    "print(SUBTLEXus_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20a1f529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['He', 'did', ',', 'however', ',', 'propel', 'himself', 'full', 'force', 'into', 'an', 'unoccupied', 'table', ',', 'bringing', 'it', 'crashing', 'to', 'the', 'ground', ',', 'its', 'umbrella', ',', 'a', 'chair', ',', 'cutlery', ',', 'salt', 'and', 'pepper', 'shakers', ',', 'packets', 'of', 'sugar', ',', 'and', 'other', 'odds', 'and', 'ends', 'falling', 'on', 'top', 'of', 'him', '.'], ['\"', 'You', 'told', 'someone', '.'], ['Despite', 'the', 'gun', 'in', 'his', 'face', ',', 'Corbett', 'remains', 'calm', '.'], ['\"', 'I', 'waited', 'as', 'he', 'savored', 'another', 'squirt', ',', 'knowing', 'he', 'had', \"n't\", 'p39', 'root', 'of', 'questions', '.'], ['\"', 'But', 'those', 'seeds', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(LM_training_corpus[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d5bac9",
   "metadata": {},
   "source": [
    "## Task 2a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdde66a3",
   "metadata": {},
   "source": [
    "After reading in the files as Pandas DataFrames (mind how you import default strings, some of them may turn into NAs!), find which token contains a typo in the given sentences, where having a typo means that the word does not feature in SUBTLEXus - remember to tokenise the input sentences! There is one and only one word containing a typo in each sentence, as defined in this way: the typo can result from the insertion/deletion/substitution of one or two characters. \n",
    "\n",
    "You should submit a .json file containing a simple dictionary mapping the sentence ID (as an integer) to the mistyped word (as a string). 5 points available in total: for every incorrect word retrieved, you lose 0.5 point. If you retrieve 10 wrong mistyped words, you get no points even if the total number of mistyped words is higher than 10. If you submit a wrongly formatted file, you will get no points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96402a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find mistyped words in the input sentences\n",
    "\n",
    "import string\n",
    "\n",
    "SUBTLEXus_words = set(SUBTLEXus_df[\"Word\"])\n",
    "\n",
    "mistyped_words = defaultdict(str)\n",
    "\n",
    "def lexicon_lookup(tokens):\n",
    "    for token in tokens:\n",
    "        if token not in SUBTLEXus_words and token not in string.punctuation:\n",
    "            return token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20bfae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in sentences_with_typos_df.iterrows():\n",
    "    sentence_id = row[\"ID\"]\n",
    "    sentence = row[\"sentence\"]\n",
    "    \n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    #print(\"index: {}, row: {}\".format(index, row), tokens)\n",
    "    \n",
    "    mistyped_words[sentence_id] = lexicon_lookup(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "17f99fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'str'>, {1: 'knoq', 2: 'reserachers', 3: 'grozn', 4: 'quolg', 5: 'waies', 6: 'wintr', 7: 'munors', 8: 'surgicaly', 9: 'aquire', 10: 'acomodate', 11: 'dats', 12: 'collegue', 13: 'layed', 14: 'cate', 15: 'giambic'})\n"
     ]
    }
   ],
   "source": [
    "print(mistyped_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2533bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"task2a_GhaithAlSeirawan_solution.json\", \"w\") as f:\n",
    "    json.dump(mistyped_words, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c013c0f",
   "metadata": {},
   "source": [
    "## Task2b\n",
    "\n",
    "read the target sentences manually. Some of them contain another mistyped word than the one you found in task2a, but were not detected because the typo resulted in a word which appears in SUBTLEXus. Discuss how you could automatically spot those mistyped words too using CL methods and resources: what information would you need? How would you use it? Reply in no more than 150 words. 3 points available, awarded based on how sensible the answer is.\n",
    "\n",
    "### Answer\n",
    "We can use a statistical language model (LM) trained on a big, corpus (such the provided LM_trainingCorpus.json) to automatically detect mistyped words even when they do not exist in SUBTLEXus. \n",
    "An LM can assist in locating words that are probably mistyped by making use of the context of the words around them. Thus, we can compute the perplexity or the probability of each word in the sentence using a trained language model on a large corpus. If a word has a high perplexity or lower probability than others in the same context we could indicate it as mistyped word. A threshold can be set to improve detection based on the distribution of perplexities or probabilities in the corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555b126e",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d98580",
   "metadata": {},
   "source": [
    "find the words from SUBTLEXus with the smallest edit distance from each mistyped target (do not lowercase anything). You should return the 3 words at the smallest edit distance, sorted by edit distance. However, if there are more words at the same edit distance than the third closest, you should include all the words at the same edit distance.\n",
    "\n",
    "    Therefore, supposing that the string 'abcdef' has two neighbors at edit distance 1, and four neighbors at edit distance 2, the third closest neighbor would be at edit distance 2, but there would be other three words at the same distance and you should thus return six neighbors for the target string. \n",
    "5 points available: you loose 0.5 points for every wrong list of nearest neighbors you retrieve for a mistyped word (wrong means any of wrong words, wrong order, wrong edit distances, wrong data type).\n",
    "Submit a .json file containing a dictionary mapping sentence IDs (as integers) to lists of tuples, where each tuple contains first the word (as a string) and then the edit distance (as an integer), with tuples sorted by edit distance in ascending order (smallest edit distance first). The dictionary should have the following form (if you submit a wrongly formatted file, you will get no points): {id1: [(w1, 2), (w2, 2), (w3, 2)]; id2: [(w6, 1), (w7, 1), (w8, 2), (w9, 2), (w10, 2)]; ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c42f26ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the nearest neighbors based on edit distance\n",
    "\n",
    "closest_neighbors = defaultdict(list)\n",
    "\n",
    "def find_edit_distance(mistyped_word):\n",
    "    target_words = list()\n",
    "    \n",
    "    for target_word in SUBTLEXus_words:\n",
    "        target_words.append((target_word, nltk.edit_distance(mistyped_word, target_word)))\n",
    "        \n",
    "    return target_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adadc67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sen_id, mistyped_word in mistyped_words.items():\n",
    "    distances = find_edit_distance(mistyped_word)\n",
    "    #print(distances[:20])\n",
    "    distances.sort(key = lambda x: x[1])\n",
    "    \n",
    "    top_neighbors = [distances[0], distances[1], distances[2]]\n",
    "    #print(\"top_neighbors \", top_neighbors)\n",
    "    \n",
    "    smallest_dist = distances[2][1]\n",
    "    #print(\"smallest_dist \", smallest_dist)\n",
    "    \n",
    "    for dist in distances[3:]:\n",
    "        if dist[1] == smallest_dist:\n",
    "            top_neighbors.append(dist)\n",
    "\n",
    "    closest_neighbors[int(sen_id)] = top_neighbors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "990df7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {1: [('know', 1), ('knot', 1), ('knob', 1)], 2: [('researchers', 2), ('researcher', 3), ('reachers', 3), ('researches', 3)], 3: [('grown', 1), ('groin', 1), ('groan', 1)], 4: [('quoit', 2), ('quote', 2), ('quota', 2), ('qualm', 2), ('quoad', 2), ('quos', 2), ('quila', 2), ('quilt', 2), ('quo', 2), ('quell', 2)], 5: [('waifs', 1), ('wails', 1), ('wakes', 1), ('waits', 1), ('wares', 1), ('waives', 1), ('wades', 1), ('wanes', 1), ('waves', 1), ('waxes', 1), ('wages', 1)], 6: [('wintry', 1), ('winter', 1), ('wants', 2), ('width', 2), ('dinar', 2), ('wanty', 2), ('dint', 2), ('winds', 2), ('contr', 2), ('tints', 2), ('entr', 2), ('wite', 2), ('diner', 2), ('wing', 2), ('wilts', 2), ('intro', 2), ('tint', 2), ('wiper', 2), ('minty', 2), ('wist', 2), ('linty', 2), ('Pinto', 2), ('bint', 2), ('wiser', 2), ('wined', 2), ('wilt', 2), ('wink', 2), ('winder', 2), ('aint', 2), ('with', 2), ('Sinter', 2), ('hints', 2), ('wit', 2), ('wintery', 2), ('wiener', 2), ('liner', 2), ('want', 2), ('win', 2), ('oint', 2), ('hint', 2), ('winker', 2), ('mint', 2), ('wine', 2), ('wont', 2), ('pints', 2), ('witty', 2), ('pint', 2), ('wins', 2), ('Pinta', 2), ('minor', 2), ('winks', 2), ('went', 2), ('into', 2), ('wits', 2), ('winos', 2), ('wince', 2), ('winery', 2), ('inter', 2), ('winch', 2), ('finer', 2), ('mints', 2), ('wind', 2), ('windy', 2), ('whiner', 2), ('lint', 2), ('Wint', 2), ('wings', 2), ('wino', 2), ('Winer', 2), ('ninth', 2), ('wines', 2), ('winner', 2), ('int', 2), ('wider', 2), ('miner', 2)], 7: [('minors', 1), ('Tuners', 2), ('rumors', 2), ('manor', 2), ('monos', 2), ('juniors', 2), ('honors', 2), ('tumors', 2), ('mayors', 2), ('miners', 2), ('Senors', 2), ('majors', 2), ('minor', 2), ('moors', 2), ('lunars', 2), ('mucous', 2), ('donors', 2), ('mentors', 2), ('manos', 2), ('humors', 2), ('tutors', 2), ('Manors', 2), ('tenors', 2), ('jurors', 2)], 8: [('surgical', 1), ('surgically', 1), ('survival', 3), ('strictly', 3), ('survivals', 3)], 9: [('quire', 1), ('acquire', 1), ('Squire', 1)], 10: [('accomodate', 1), ('accommodate', 2), ('accommodated', 3), ('abominate', 3), ('accommodates', 3)], 11: [('dais', 1), ('Kats', 1), ('date', 1), ('cats', 1), ('dads', 1), ('gats', 1), ('dates', 1), ('dots', 1), ('hats', 1), ('data', 1), ('Lats', 1), ('wats', 1), ('pats', 1), ('bats', 1), ('rats', 1), ('days', 1), ('tats', 1), ('mats', 1), ('doats', 1), ('dams', 1), ('dato', 1), ('eats', 1), ('darts', 1), ('fats', 1), ('vats', 1), ('dabs', 1), ('oats', 1)], 12: [('college', 1), ('colleague', 1), ('collage', 2), ('colleges', 2), ('colleagues', 2)], 13: [('laced', 1), ('layer', 1), ('lated', 1), ('played', 1), ('lazed', 1), ('flayed', 1), ('lamed', 1), ('payed', 1), ('Fayed', 1), ('slayed', 1)], 14: [('caste', 1), ('date', 1), ('cats', 1), ('came', 1), ('late', 1), ('cage', 1), ('cat', 1), ('cute', 1), ('sate', 1), ('mate', 1), ('cave', 1), ('Cate', 1), ('pate', 1), ('cafe', 1), ('ate', 1), ('case', 1), ('cite', 1), ('care', 1), ('rate', 1), ('hate', 1), ('fate', 1), ('cake', 1), ('bate', 1), ('cane', 1), ('crate', 1), ('gate', 1), ('carte', 1), ('yate', 1), ('Tate', 1), ('cater', 1)], 15: [('iambic', 1), ('gambit', 2), ('limbic', 2), ('iambics', 2)]})\n"
     ]
    }
   ],
   "source": [
    "print(closest_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ed9de3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"task3_GhaithAlSeirawan_solution.json\", \"w\") as f:\n",
    "    json.dump(closest_neighbors, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f911707",
   "metadata": {},
   "source": [
    "## Task4: frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3831bf79",
   "metadata": {},
   "source": [
    "Use the list of candidate replacements you found in task3 to find the best one according to candidate frequency (derived from SUBTLEXus)\n",
    "\n",
    "- If two or more candidates have the exact same frequency in SUBTLEX, choose the one with the best edit distance. \n",
    "- If two or more candidates at the same frequency also have the same edit distance, pick the one which comes first in alphabetical order.\n",
    "\n",
    "You should return a .json file containing a simple dictionary mapping the sentence ID (as an integer) to a tuple containing the best candidate replacement (as a string) and its SUBTLEXus frequency value (as an integer). 5 points available, you lose 1 point for each wrong best candidate:frequency pair you retrieve (if the candidate is right but the frequency doesn't match, you loose half a point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e697fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just a helping function for debugging\n",
    "from itertools import islice\n",
    "\n",
    "def take(n, iterable):\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "00b809de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the best candidate according to SUBTLEXus frequency counts\n",
    "\n",
    "# I will be using \"Lg10WF\" for this task instead of computing the frequency counts manually.\n",
    "# The \"Lg10WF\" is log10(FREQcount+1) according to https://www.ugent.be/pp/experimentele-psychologie/en/research/documents/subtlexus\n",
    "\n",
    "\n",
    "SUBTLEXus_freq = SUBTLEXus_df.set_index(\"Word\")[\"Lg10WF\"].to_dict()\n",
    "\n",
    "# print(take(5, SUBTLEXus_freq))\n",
    "# print(SUBTLEXus_freq['the'])\n",
    "# print(SUBTLEXus_freq.get('the'))\n",
    "\n",
    "\n",
    "def find_best_candidate_WF(neighbors, lexicon_WF):\n",
    "    best_candidate = None\n",
    "    best_frequency = -1\n",
    "    best_edit_distance = float(\"inf\")\n",
    "\n",
    "    for candidate, edit_distance in neighbors:\n",
    "        frequency = lexicon_WF.get(candidate)\n",
    "\n",
    "        if (\n",
    "            (frequency > best_frequency)\n",
    "            or (frequency == best_frequency and edit_distance < best_edit_distance) \n",
    "            or (\n",
    "                frequency == best_frequency\n",
    "                and edit_distance == best_edit_distance\n",
    "                and candidate < best_candidate\n",
    "               )\n",
    "           ):\n",
    "            \n",
    "            best_candidate = candidate\n",
    "            best_frequency = frequency\n",
    "            best_edit_distance = edit_distance\n",
    "            \n",
    "    return (best_candidate, best_frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8defe0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_candidate_replacements = {}\n",
    "\n",
    "for sen_id, neighbors in closest_neighbors.items():\n",
    "    best_candidate_replacements[sen_id] = find_best_candidate_WF(neighbors, SUBTLEXus_freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e2ac88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ('know', 5.4651), 2: ('researchers', 1.7634), 3: ('grown', 3.1059), 4: ('quote', 2.6893), 5: ('waves', 2.8293), 6: ('with', 5.4107), 7: ('minor', 2.8162), 8: ('strictly', 2.7396), 9: ('Squire', 2.1987), 10: ('accommodate', 2.0414), 11: ('days', 4.1929), 12: ('college', 3.638), 13: ('played', 3.458), 14: ('care', 4.3936), 15: ('gambit', 1.301)}\n"
     ]
    }
   ],
   "source": [
    "print(best_candidate_replacements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6fe79361",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"task4_GhaithAlSeirawan_solution.json\", \"w\") as f:\n",
    "    json.dump(best_candidate_replacements, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024d5b2",
   "metadata": {},
   "source": [
    "## Task5: perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fd7f4d",
   "metadata": {},
   "source": [
    "Use the list of candidate replacements you found in task3 to find the best one according to its perplexity under a statistical language model of order 3 implemented using a Markov Chain with add-k smoothing (k=0.01) and estimated using the given corpus. \n",
    "\n",
    "- If two or more candidates have the exact same perplexity in the input sentence, choose the one with the best edit distance. \n",
    "- If two or more candidates at the same perplexity also have the same edit distance, follow alphabetical order.\n",
    "\n",
    "You should return a .json file containing a simple dictionary mapping the sentence ID (as an integer) to a tuple containing the best candidate replacement (as a string) and its perplexity under the specified language model (as a float). Not all candidate replacements might appear in the LM training corpus: don't map such candidate replacements to the UNK string though, or the perplexity estimate would not reflect the specific candidate; you can directly exclude candidate replacements which don't appear in the training corpus from the perplexity computation. \n",
    "\n",
    "5 points available, you lose 1 point for each wrong best candidate:frequency pair you retrieve (if the candidate is right but the perplexity doesn't match, you loose half a point)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4bc2e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the best candidate according to perplexity under the given language model\n",
    "\n",
    "lm = LM(LM_training_corpus, ngram_size=3, k=0.01)\n",
    "lm.update_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "174debc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_words(sentence, old_word, new_word):\n",
    "    tokens = nltk.tokenize.word_tokenize(sentence)\n",
    "    index = tokens.index(old_word)\n",
    "    tokens[index] = new_word\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e714188d",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_candidates_perplexity = {}\n",
    "\n",
    "for sen_id, neighbors in closest_neighbors.items():\n",
    "    sentence = sentences_with_typos_df.loc[sentences_with_typos_df[\"ID\"] == int(sen_id), \"sentence\"].iloc[0]\n",
    "    mistyped_word = mistyped_words[sen_id]\n",
    "\n",
    "    best_candidate = None\n",
    "    best_perplexity = float(\"inf\")\n",
    "    \n",
    "    for word, edit_distance in neighbors:\n",
    "        if word in lm.vocab:\n",
    "            replaced_sentence = replace_words(sentence, mistyped_word, word)\n",
    "            perplexity = lm.perplexity(replaced_sentence)\n",
    "\n",
    "            if perplexity < best_perplexity:\n",
    "                best_perplexity = perplexity\n",
    "                best_candidate = (word, edit_distance)\n",
    "            elif perplexity == best_perplexity:\n",
    "                if (edit_distance < best_candidate[1]) or (edit_distance == best_candidate[1] and word < best_candidate[0]):\n",
    "                    best_candidate = (word, edit_distance)\n",
    "\n",
    "    best_candidates_perplexity[sen_id] = (best_candidate[0], best_perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bf43d6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ('know', 1002.7288999720108), 2: ('researchers', 14475.415924638553), 3: ('groin', 1858.1347148669051), 4: ('quote', 16820.32265338231), 5: ('waves', 1220.1171486647056), 6: ('wind', 1857.4043179891894), 7: ('jurors', 9336.433652424052), 8: ('survival', 18918.657040618258), 9: ('Squire', 2190.000743626073), 10: ('accommodate', 1897.8469004552046), 11: ('date', 2274.1923660774823), 12: ('colleague', 888.999722318408), 13: ('Fayed', 6625.174599990876), 14: ('care', 11882.810558938134), 15: ('iambic', 31291.168009478923)}\n"
     ]
    }
   ],
   "source": [
    "print(best_candidates_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e5ad6432",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"task5_GhaithAlSeirawan_solution.json\", \"w\") as f:\n",
    "    json.dump(best_candidates_perplexity, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8513369b",
   "metadata": {},
   "source": [
    "## Task 6\n",
    "\n",
    "Compare the candidate replacements found when considering frequency and when considering perplexity. Which are better? Do they match what you consider to be the right replacement? What extra resources/information would you use to pick better candidate replacements? 3 points available, awarded based on how sensible the answer is.\n",
    "\n",
    "### Note: the answer after one cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c5c54ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Perplexity</th>\n",
       "      <th>Grammerly.com</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>know</td>\n",
       "      <td>know</td>\n",
       "      <td>know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>researchers</td>\n",
       "      <td>researchers</td>\n",
       "      <td>researchers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grown</td>\n",
       "      <td>groin</td>\n",
       "      <td>groan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>quote</td>\n",
       "      <td>quote</td>\n",
       "      <td>quilt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>waves</td>\n",
       "      <td>waves</td>\n",
       "      <td>waves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>with</td>\n",
       "      <td>wind</td>\n",
       "      <td>winter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>minor</td>\n",
       "      <td>jurors</td>\n",
       "      <td>minors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>strictly</td>\n",
       "      <td>survival</td>\n",
       "      <td>surgically</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Squire</td>\n",
       "      <td>Squire</td>\n",
       "      <td>acquire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>accommodate</td>\n",
       "      <td>accommodate</td>\n",
       "      <td>acconmmodate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>days</td>\n",
       "      <td>date</td>\n",
       "      <td>dates</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>college</td>\n",
       "      <td>colleague</td>\n",
       "      <td>colleague</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>played</td>\n",
       "      <td>Fayed</td>\n",
       "      <td>laid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>care</td>\n",
       "      <td>care</td>\n",
       "      <td>care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>gambit</td>\n",
       "      <td>iambic</td>\n",
       "      <td>iambic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Frequency   Perplexity Grammerly.com\n",
       "ID                                        \n",
       "1          know         know          know\n",
       "2   researchers  researchers   researchers\n",
       "3         grown        groin         groan\n",
       "4         quote        quote         quilt\n",
       "5         waves        waves         waves\n",
       "6          with         wind        winter\n",
       "7         minor       jurors        minors\n",
       "8      strictly     survival    surgically\n",
       "9        Squire       Squire       acquire\n",
       "10  accommodate  accommodate  acconmmodate\n",
       "11         days         date         dates\n",
       "12      college    colleague     colleague\n",
       "13       played        Fayed          laid\n",
       "14         care         care          care\n",
       "15       gambit       iambic        iambic"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the found words for conveniance \n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "candidate_replacements = {\n",
    "    \"ID\": [],\n",
    "    \"Frequency\": [],\n",
    "    \"Perplexity\": [],\n",
    "    \"Grammerly.com\": [\"know\", \"researchers\", \"groan\", \"quilt\", \"waves\", \"winter\", \"minors\", \"surgically\", \"acquire\", \"acconmmodate\", \"dates\", \"colleague\", \"laid\", \"care\", \"iambic\"]\n",
    "}\n",
    "\n",
    "for i in best_candidate_replacements:\n",
    "    candidate_replacements[\"ID\"].append(int(i))\n",
    "    candidate_replacements[\"Frequency\"].append(best_candidate_replacements[i][0])\n",
    "    candidate_replacements[\"Perplexity\"].append(best_candidates_perplexity[i][0])\n",
    "    \n",
    "candidate_replacements_df = pd.DataFrame(candidate_replacements)\n",
    "candidate_replacements_df.set_index(\"ID\", inplace = True)\n",
    "\n",
    "display(candidate_replacements_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868b4db",
   "metadata": {},
   "source": [
    "### Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0761112b",
   "metadata": {},
   "source": [
    "Comparing the candidate replacements found, I observed that some replacements are better than others. I manually checked the sentences for typos using Grammarly.com to compare both replacements found using frequency and perplexity. Only six words of the found replacements matched between frequency and perplexity. Perplexity replacements seem better in some cases, e.g., sentence 11: \"data\" instead of \"days\" or sentence 15: \"iambic\" instead of \"gambit.\" \n",
    "\n",
    "Compared to Grammarly.com, both methods have limitations and need to be improved in finding the correct replacement. e.g., sentence 4: Grammarly suggested \"quilt\" but the frequency and perplexity \"quote\" or, sentence 7: Grammarly suggested \"minors\" but freq and perplexity \"minor,\" \"jurors\" respectively.\n",
    "\n",
    "A better replacement candidate might result from combining frequency and perplexity. This can be done by constructing a score measure defined as the product of given weights for the computed frequency and perplexity and multiplying by them. Considering both frequency and perplexity when choosing the best candidate replacements may lead to better results.\n",
    "\n",
    "Given the volume of their training data, more advanced language models with BERT or GPT-based may also be able to provide better contextualized suggestions. Another method, like the Word@vec algorithm, that predicts the correct word using semantic similarity measures could also provide more accurate replacements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f674914d",
   "metadata": {},
   "source": [
    "#### The end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
